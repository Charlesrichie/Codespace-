{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lqf2Q2Cmj5sy"},"outputs":[],"source":["# utilities\n","import re\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import io\n","\n","# plotting\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","# nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","# sklearn\n","from sklearn.svm import LinearSVC\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import confusion_matrix, classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"vHesC0lmyPi1"},"outputs":[{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-a2c81670-b1b0-4afe-b3cb-c3fbba3baaa4\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-a2c81670-b1b0-4afe-b3cb-c3fbba3baaa4\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript src=\"/nbextensions/google.colab/files.js\"\u003e\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving Sentiment140.tenPercent.sample.tweets.csv to Sentiment140.tenPercent.sample.tweets (1).csv\n"]}],"source":["# import tsv file to google colab\n","from google.colab import files\n","uploaded = files.upload()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Srb0j9tyj57T","outputId":"44229300-cebe-4afe-d7c4-8d477de26e0a"},"outputs":[{"ename":"KeyError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-5-3d767577a2dd\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDATASET_ENCODING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'South Africa.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET_ENCODING\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Dataset is now stored in a Pandas Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'South Africa.csv'"]}],"source":["#read the tsv file using pandas\n","# Importing the dataset\n","DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n","DATASET_ENCODING = \"ISO-8859-1\"\n","\n","dataset = pd.read_csv(io.BytesIO(uploaded['South Africa.csv'], encoding=DATASET_ENCODING , names=DATASET_COLUMNS))\n","# Dataset is now stored in a Pandas Dataframe\n","\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":1019,"status":"error","timestamp":1644311984037,"user":{"displayName":"Charles Oredola","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnpwCalP23D3Pr8M6WnJ42nMBIpm7KfjhgDPqn=s64","userId":"14286154376090524793"},"user_tz":-60},"id":"8UIctFkfoOvJ","outputId":"bd958731-8089-473a-b106-a09b2f44a5a2"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-32ad2582adae\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDATASET_ENCODING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n\u001b[0;32m----\u003e 5\u001b[0;31m                       encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Removing the unnecessary columns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--\u003e 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/sentiment140/training.1600000.processed.noemoticon.csv'"]}],"source":["# Importing the dataset\n","DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n","DATASET_ENCODING = \"ISO-8859-1\"\n","dataset = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n","                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n","\n","# Removing the unnecessary columns.\n","dataset = dataset[['sentiment','text']]\n","# Replacing the values to ease understanding.\n","dataset['sentiment'] = dataset['sentiment'].replace(4,1)\n","\n","# Plotting the distribution for dataset.\n","ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n","                                               legend=False)\n","ax.set_xticklabels(['Negative','Positive'], rotation=0)\n","\n","# Storing data in lists.\n","text, sentiment = list(dataset['text']), list(dataset['sentiment'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6i5r-LEkrjfA"},"outputs":[],"source":["# Defining dictionary containing all emojis with their meanings.\n","emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n","          ':-(': 'sad', ':-\u003c': 'sad', ':P': 'raspberry', ':O': 'surprised',\n","          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n","          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-\u0026': 'confused', '$_$': 'greedy',\n","          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n","          '\u003c(-_-)\u003e': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n","          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n","\n","## Defining set containing all stopwords in english.\n","stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n","             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n","             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n","             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n","             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n","             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n","             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n","             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n","             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n","             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n","             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n","             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n","             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n","             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n","             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n","             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0e-F3kffr6lD"},"outputs":[],"source":["def preprocess(textdata):\n","    processedText = []\n","    \n","    # Create Lemmatizer and Stemmer.\n","    wordLemm = WordNetLemmatizer()\n","    \n","    # Defining regex patterns.\n","    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n","    userPattern       = '@[^\\s]+'\n","    alphaPattern      = \"[^a-zA-Z0-9]\"\n","    sequencePattern   = r\"(.)\\1\\1+\"\n","    seqReplacePattern = r\"\\1\\1\"\n","    \n","    for tweet in textdata:\n","        tweet = tweet.lower()\n","        \n","        # Replace all URls with 'URL'\n","        tweet = re.sub(urlPattern,' URL',tweet)\n","        # Replace all emojis.\n","        for emoji in emojis.keys():\n","            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n","        # Replace @USERNAME to 'USER'.\n","        tweet = re.sub(userPattern,' USER', tweet)        \n","        # Replace all non alphabets.\n","        tweet = re.sub(alphaPattern, \" \", tweet)\n","        # Replace 3 or more consecutive letters by 2 letter.\n","        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n","\n","        tweetwords = ''\n","        for word in tweet.split():\n","            # Checking if the word is a stopword.\n","            #if word not in stopwordlist:\n","            if len(word)\u003e1:\n","                # Lemmatizing the word.\n","                word = wordLemm.lemmatize(word)\n","                tweetwords += (word+' ')\n","            \n","        processedText.append(tweetwords)\n","        \n","    return processedText"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2Tz33G4tEpy"},"outputs":[],"source":["import time\n","t = time.time()\n","processedtext = preprocess(text)\n","print(f'Text Preprocessing complete.')\n","print(f'Time Taken: {round(time.time()-t)} seconds')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTAu7MGEr546"},"outputs":[],"source":["data_neg = processedtext[:800000]\n","plt.figure(figsize = (15,15))\n","wc = WordCloud(max_words = 500 , width = 1000 , height = 500,\n","               collocations=False).generate(\" \".join(data_neg))\n","plt.imshow(wc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfBnwpYNt_1w"},"outputs":[],"source":["data_pos = processedtext[800000:]\n","wc = WordCloud(max_words = 500 , width = 1000 , height = 500,\n","              collocations=False).generate(\" \".join(data_pos))\n","plt.figure(figsize = (15,15))\n","plt.imshow(wc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTaJXpi8uVzt"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n","                                                    test_size = 0.10, random_state = 2)\n","print(f'Data Split done.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45oO0KrWuuJb"},"outputs":[],"source":["vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n","vectoriser.fit(X_train)\n","print(f'Vectoriser fitted.')\n","print('No. of feature_words: ', len(vectoriser.get_feature_names()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AkRLX60Sut1m"},"outputs":[],"source":["X_train = vectoriser.transform(X_train)\n","X_test  = vectoriser.transform(X_test)\n","print(f'Data Transformed.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_n1tg9nuteO"},"outputs":[],"source":["def model_Evaluate(model):\n","    \n","    # Predict values for Test dataset\n","    y_pred = model.predict(X_test)\n","\n","    # Print the evaluation metrics for the dataset.\n","    print(classification_report(y_test, y_pred))\n","    \n","    # Compute and plot the Confusion matrix\n","    cf_matrix = confusion_matrix(y_test, y_pred)\n","\n","    categories  = ['Negative','Positive']\n","    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n","    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n","\n","    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n","    labels = np.asarray(labels).reshape(2,2)\n","\n","    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n","                xticklabels = categories, yticklabels = categories)\n","\n","    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n","    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n","    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ykWSjmyuswc"},"outputs":[],"source":["BNBmodel = BernoulliNB(alpha = 2)\n","BNBmodel.fit(X_train, y_train)\n","acc_BNB= model_Evaluate(BNBmodel)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KA1y2qPqILHF"},"outputs":[],"source":["SVCmodel = LinearSVC()\n","SVCmodel.fit(X_train, y_train)\n","acc_SVC= model_Evaluate(SVCmodel)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJnGMY35IMid"},"outputs":[],"source":["LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n","LRmodel.fit(X_train, y_train)\n","acc_LR= model_Evaluate(LRmodel)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbvVQv7OIMRm"},"outputs":[],"source":["file = open('vectoriser-ngram-(1,2).pickle','wb')\n","pickle.dump(vectoriser, file)\n","file.close()\n","\n","file = open('Sentiment-LR.pickle','wb')\n","pickle.dump(LRmodel, file)\n","file.close()\n","\n","file = open('Sentiment-BNB.pickle','wb')\n","pickle.dump(SVCmodel, file)\n","file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRt3DKD5IMFT"},"outputs":[],"source":["def load_models():\n","    '''\n","    Replace '..path/' by the path of the saved models.\n","    '''\n","    \n","    # Load the vectoriser.\n","    file = open('..path/vectoriser-ngram-(1,2).pickle', 'rb')\n","    vectoriser = pickle.load(file)\n","    file.close()\n","    # Load the LR Model.\n","    file = open('..path/Sentiment-LRv1.pickle', 'rb')\n","    LRmodel = pickle.load(file)\n","    file.close()\n","    \n","    return vectoriser, LRmodel\n","\n","def predict(vectoriser, model, text):\n","    # Predict the sentiment\n","    textdata = vectoriser.transform(preprocess(text))\n","    sentiment = model.predict(textdata)\n","    \n","    # Make a list of text with sentiment.\n","    data = []\n","    for text, pred in zip(text, sentiment):\n","        data.append((text,pred))\n","        \n","    # Convert the list into a Pandas DataFrame.\n","    df = pd.DataFrame(data, columns = ['text','sentiment'])\n","    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n","    return df\n","\n","if __name__==\"__main__\":\n","    # Loading the models.\n","    #vectoriser, LRmodel = load_models()\n","    \n","    # Text to classify should be in a list.\n","    text = [\"This minimie tastes weird\",\n","            \"Dogecoin is a big flop\",\n","            \"I like eating catfish\",\n","           \"Bigi drinks is cheaper and tastes the same as coke\"]\n","    \n","    df = predict(vectoriser, LRmodel, text)\n","    print(df.head())"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMEwSUcByHbHxoQLRfsL2P4","name":"Untitled7.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}